# Registered Filter Plugins

The filter identifier is designed to be a unique identifier for the filter. Values from zero through 32,767 are reserved for filters supported by The HDF Group in the HDF5 library and for filters requested and supported by the 3rd party.

Values from 32768 to 65535 are reserved for non-distributed uses (e.g., internal company usage) or for application usage when testing a feature. The HDF Group does not track or document the usage of filters with identifiers from this range.

Please contact the maintainer of a filter for help with the plugin.

| WARNING: be aware that compression filters require that the library not use `H5_MEMORY_ALLOC_SANITY_CHECK`.  Building in debug mode automatically enables this feature in earlier releases, which causes memory allocation and free problems in filter applications. Current versions of HDF5 do not enable this feature. |
| :--- |

| Filter | ID  | Description | URL | Contact |
| ---    | --- | ---         | --- | ---     |
| LZO  | 305 | **LZO lossless compression used by PyTables** <ul><li>LZO is a portable lossless data compression library written in ANSI C.</li><li>Reliable and thoroughly tested. High adoption - each second terrabytes of data are compressed by LZO. No bugs since the first release back in 1996.</li><li>Offers pretty fast compression and *extremely* fast decompression.</li><li>Includes slower compression levels achieving a quite competitive compression ratio while still decompressing at this very high speed.</li><li>Distributed under the terms of the GNU General Public License (GPL v2+). Commercial licenses are available on request.</li><li>Military-grade stability and robustness.</li></ul> | [LZO](http://www.oberhumer.com/opensource/lzo/), [PyTables](http://www.pytables.org/) | [Francesc Alted](mailto:faltet@pytables.org) |
| BZIP2 | 307 | **BZIP2 lossless compression used by PyTables** bzip2 is a freely available, patent free, high-quality data compressor. It typically compresses files to within 10% to 15% of the best available techniques (the PPM family of statistical compressors), whilst being around twice as fast at compression and six times faster at decompression. | [bzip2](http://www.bzip.org/), [PyTables](http://www.pytables.org/) |  [Francesc Alted](mailto:faltet@pytables.org) |
| LZF | 32000 | **LZF lossless compression used by H5Py project** The LZF filter is an alternative DEFLATE-style compressor for HDF5 datasets, using the free LZF library by Marc Alexander Lehmann. Its main benefit over the built-in HDF5 DEFLATE filter is speed; in memory-to-memory operation as part of the filter pipeline, it typically compresses 3x-5x faster than DEFLATE, and decompresses 2x faster, while maintaining 50% to 90% of the DEFLATE compression ratio. LZF can be used to compress any data type, and requires no compile-time or run-time configuration. HDF5 versions 1.6.5 through 1.8.3 are supported. The filter is written in C and can be included directly in C or C++ applications; it has no external dependencies. The license is 3-clause BSD (virtually unrestricted, including commercial applications). The LZF filter was developed as part of the `h5py` project, which implements a general-purpose interface to HDF5 from Python. More information, downloads, and benchmarks, are available [here](http://h5py.org/lzf/). | [`h5py`](http://h5py.org), [LZF](http://home.schmorp.de/marc/liblzf.html) | [Andrew Collette](http://h5py.org) |
| BLOSC | 32001 | **Blosc lossless compression used by PyTables** Blosc is a high performance compressor optimized for binary data. It has been designed to compress data very fast, at the expense of achieving lesser compression ratios than, say, zlib+shuffle. It is mainly meant to not introduce a significant delay when dealing with data that is stored in high-performance I/O systems (like large RAID cabinets, or even the OS filesystem memory cache). It uses advanced cache-efficient techniques to reduce activity on the memory bus as much as possible. It also leverages SIMD (SSE2) and multi-threading capabilities present in nowadays multi-core processors so as to accelerate the compression/decompression process to a maximum. | [blosc](http://blosc.org/), [PyTables](http://www.pytables.org/) |  [Francesc Alted](mailto:faltet@pytables.org) |
| MAFISC | 32002 | **Modified LZMA compression filter, MAFISC (Multidimensional Adaptive Filtering Improved Scientific data Compression)** This compressing filter exploits the multidimensionality and smoothness characterizing many scientific data sets. It adaptively applies some filters to preprocess the data and uses lzma as the actual compression step. It significantly outperforms pure lzma compression on most datasets. The software is currently under a rather unrestrictive two clause BSD style license. | [Link](http://wr.informatik.uni-hamburg.de/research/projects/icomex/mafisc) | [Nathanael Huebbe](mailto:nathanael.huebbe@informatik.uni-hamburg.de)
| Snappy | 32003 | **Snappy lossless compression** Snappy-CUDA is a compression/decompression library that leverages GPU processing power to compress/decompress data. The Snappy compression algorithm does not aim for maximum compression or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. For instance, compared to the fastest mode of zlib, the reference implementation of Snappy on the CPU is an order of magnitude faster for most inputs, but the resulting compressed files are anywhere from 20% to 100% bigger. | [snappy-cuda](https://github.com/lucasvr/snappy-cuda), [snappy](https://github.com/google/snappy) | [Lucas C. Villa Real](mailto:lucasvr@gmail.com)
| LZ4 | 32004 | **LZ4 fast lossless compression** LZ4 is a very fast lossless compression algorithm, providing compression speed at 300 MB/s per core, scalable with multi-cores CPU. It also features an extremely fast decoder, with speeds up and beyond 1GB/s per core, typically reaching RAM speed limits on multi-core systems. For a format description of the LZ4 compression filter in HDF5, see [here](https://support.hdfgroup.org/services/filters/HDF5_LZ4.pdf). | [Source](https://github.com/nexusformat/HDF5-External-Filter-Plugins/tree/master/LZ4) | [Michael Rissi (Dectris Ltd.)](mailto:michael.rissi@dectris.com) |
| APAX | 32005 | **Samplify's APAX Numerical Encoding Technology** _Appears to be no longer available_ | | |
| CBF | 32006 | All imgCIF/CBF compressions and decompressions, including Canonical, Packed, Packed Version 2, Byte Offset and Nibble Offset. License Information: GPL and LGPL | | [Herbert J. Bernstein](mailto:yayahjb@gmail.com) |
| JPEG-XR | 32007 | **Image JPEG-XR compression** Filter that allows HDF5 image datasets to be compressed or decompressed using the JPEG-XR compression method. | [Link](https://en.wikipedia.org/wiki/JPEG_XR) | [Marvin Albert](mailto:marvin.albert@gmail.com) |
| bitshuffle | 32008 | **Extreme version of shuffle filter that shuffles data at bit level instead of byte level** This filter shuffles data at the bit level to improve compression. CHIME uses this filter for data acquisition. | [bitshuffle](https://github.com/kiyo-masui/bitshuffle), [CHIME](http://chime.phas.ubc.ca/) | [Kiyoshi Masui](mailto:kiyo@physics.ubc.ca) |
| SPDP | 32009 | **SPDP fast lossless compression algorithm for single- and double-precision floating-point data** SPDP is a fast, lossless, unified compression/decompression algorithm designed for both 32-bit single-precision (float) and 64-bit double-precision (double) floating-point data. It also works on other data. | [Link](http://cs.txstate.edu/~burtscher/research/SPDP/) | [Martin Burtscher](mailto:burtscher@txstate.edu) |
| LPC-Rice | 32010 | **LPC-Rice multi-threaded lossless compression** LPC-Rice is a fast lossless compression codec that employs Linear Predictive Coding together with Rice coding. It supports multi-threading and SSE2 vector instructions, enabling it to exceed compression and decompression speeds of 1 GB/s. | [Link](https://sourceforge.net/projects/lpcrice/) | [Frans van den Bergh](mailto:fvdbergh@csir.co.za), [Derick Swanepoel](mailto:dswanepoel@gmail.com) |
| CCSDS-123 | 32011	| **ESA CCSDS-123 multi-threaded compression filter** CCSDS-123 is a multi-threaded HDF5 compression filter using the ESA CCSDS-123 implementation. | [Link](https://sourceforge.net/projects/ccsds123-hdf-filter/) | [Frans van den Bergh](mailto:fvdbergh@csir.co.za), [Derick Swanepoel](mailto:dswanepoel@gmail.com) |
|	JPEG-LS |  32012 | **CharLS JPEG-LS multi-threaded compression filter** JPEG-LS is a multi-threaded HDF5 compression filter using the CharLS JPEG-LS implementation. | [Link](https://sourceforge.net/projects/jpegls-hdf-filter/) | [Frans van den Bergh](mailto:fvdbergh@csir.co.za), [Derick Swanepoel](mailto:dswanepoel@gmail.com) |
| zfp | 32013	| **Rate, accuracy or precision bounded compression for floating-point arrays** zfp is a BSD licensed open source C++ library for compressed floating-point arrays that support very high throughput read and write random access. zfp was designed to achieve high compression ratios and therefore uses lossy but optionally error-bounded compression. Although bit-for-bit lossless compression is not always possible, zfp is usually accurate to within machine epsilon in near-lossless mode, and is often orders of magnitude more accurate and faster than other lossy compressors. | [Link](https://github.com/LLNL/H5Z-ZFP), [Info](http://computation.llnl.gov/projects/floating-point-compression/) | [Mark Miller](mailto:miller86@llnl.gov), [Peter Lindstrom](mailto:pl@llnl.gov) |
| fpzip | 32014	| **Fast and Efficient Lossy or Lossless Compressor for Floating-Point Data** fpzip is a library for lossless or lossy compression of 2D or 3D floating-point scalar fields. Although written in C++, fpzip has a C interface. fpzip was developed by Peter Lindstrom at LLNL. | [Link](http://computation.llnl.gov/projects/floating-point-compression/) | [Peter Lindstrom](mailto:pl@llnl.gov) |
| Zstandard | 32015	|	**Real-time compression algorithm with wide range of compression / speed trade-off and fast decoder** Zstandard is a real-time compression algorithm, providing high compression ratios. It offers a very wide range of compression / speed trade-offs, while being backed by a very fast decoder. The Zstandard library is provided as open source software using a BSD license. | [Link](https://github.com/aparamon/HDF5Plugin-Zstandard) | [Andrey Paramonov](mailto:paramon@acdlabs.ru) |
| B³D	| 32016	| **GPU based image compression method developed for light-microscopy applications** B³D is a fast (~1 GB/s), GPU based image compression method, developed for light-microscopy applications. Alongside lossless compression, it offers a noise dependent lossy compression mode, where the loss can be tuned as a proportion of the inherent image noise (accounting for photon shot noise and camera read noise). It not only allows for fast compression during image, but can achieve compression ratios up 100. | [Link](http://www.biorxiv.org/content/early/2017/07/21/164624) | |
| SZ | 32017 | **An error-bounded lossy compressor for scientific floating-point data** SZ is a fast and efficient error-bounded lossy compressor for floating-point data. It was developed for scientific applications producing large-scale HPC data sets. SZ supports C, Fortran, and Java and has been tested on Linux and Mac OS X. | [Info](https://collab.cels.anl.gov/display/ESR/SZ), [Source](https://github.com/disheng222/SZ), [License](http://www.mcs.anl.gov/~shdi/download/sz-download.html) | [Sheng Di](mailto:sdi1@anl.gov), [Franck Cappello](mailto:cappello@mcs.anl.gov) |
| FCIDECOMP | 32018 | **EUMETSAT CharLS compression filter for use with netCDF** FCIDECOMP is a third-party compression filter used at EUMETSAT for the compression of netCDF-4 files. It is a codec implementing JPEG-LS using CharLS used for satellite imagery. | [Link](ftp://ftp.eumetsat.int/pub/OPS/out/test-data/Test-data-for-External-Users/MTG_FCI_L1c_Compressed-Datasets_and_Decompression-Plugin_April2017/Decompression_Plugin/) | [Dr. Daniel Lee](mailto:daniel.lee@eumetsat.int) |
| JPEG | 32019 | **Jpeg compression filter** This is a lossy compression filter. It provides a user-specified "quality factor" to control the trade-off of size versus accuracy. `libjpeg`: This library is available as a package for most Linux distributions, and source code is available from [here](https://www.ijg.org/). **Restrictions:** <ul><li>Only 8-bit unsigned data arrays are supported.</li><li>Arrays must be either:<ul><li>2-D monochromatic [NumColumns, NumRows]</li><li>3-D RGB [3, NumColumns, NumRows]</li></ul><li>Chunking must be set to the size of one entire image so the filter is called once for each image.</li></ul> **Using the JPEG filter in your application:** HDF5 only supports compression for "chunked" datasets; this just means that you need to call H5Pset_chunk to specify a chunk size. The chunking must be set to the size of a single image for the JPEG filter to work properly. When calling H5Pset_filter for compression it must be called with `cd_nelmts=4` and `cd_values` as follows:<ul><li><code>cd_values[0] = quality factor (1-100)</code></li><li><code>cd_values[1] = numColumns</code><li><li><code>cd_values[2] = numRows</code></li><li><code>cd_values[3] = 0=Mono, 1=RGB</code></li></ul> Common h5repack parameter: `UD=32019,0,4,q,c,r,t` | [Info](https://github.com/CARS-UChicago/jpegHDF5/blob/master/README.md), [Source](https://github.com/CARS-UChicago/jpegHDF5), [License](https://github.com/CARS-UChicago/jpegHDF5/blob/master/LICENSE) | [Mark Rivers , University of Chicago](mailto:rivers@cars.uchicago.edu) |
| VBZ | 32020	| **Compression filter for raw dna signal data used by Oxford Nanopore** This filter is used by Oxford Nanopore specifically to compress raw dna signal data (signed integer). To achieve this it uses both: <ul><li><a href="https://github.com/lemire/streamvbyte">streamvbyte</a></li><li><a href="https://github.com/facebook/zstd">zstd</a></li></ul> | | George Pimm |
| FAPEC | 32021	| **Versatile and efficient data compressor supporting many kinds of data and using an outlier-resilient entropy coder** FAPEC is a versatile and efficient data compressor, initially designed for satellite payloads but later extended for ground applications. It relies on an outlier-resilient entropy coding core with similar ratios and speeds than CCSDS 121.0 (adaptive Rice). FAPEC has a large variety of pre-processing stages and options: images (greyscale, colour, hyperspectral); time series or waveforms (including interleaving, e.g. for multidimensional or interleaved time series or tabular data); floating point (single+double precision); text (including LZW compression and our faster FAPECLZ); tabulated text (CSV); genomics (FastQ); geophysics (Kongsberg's water column datagrams); etc. Most stages support samples of 8 to 24 bits (big/little endian, signed/unsigned), and lossless/lossy options. It can be extended with new, tailored pre-processing stages. It includes encryption options (AES-256 based on OpenSSL, and our own XXTEA implementation). The FAPEC library and CLI runs on Linux, Windows and Mac. The HDF5 user must request and install the library separately, thus allowing to upgrade it without requiring changes in your HDF5 code. | [Info](https://www.dapcom.es/fapec/), [Distro](https://www.dapcom.es/get-fapec/), [License](https://www.dapcom.es/resources/FAPEC_EndUserLicenseAgreement.pdf) | [Jordi Portell i de Mora (DAPCOM Data Services S.L.)](mailto:fapec@dapcom.es) |
|	BitGroom | 32022 | **The BitGroom quantization algorithm** The BitGroom quantization algorithm is documented in: _Zender, C. S. (2016), Bit Grooming: Statistically accurate precision-preserving quantization with compression, evaluated in the netCDF Operators (NCO, v4.4.8+), Geosci. Model Dev., 9, 3199-3211, doi:10.5194/gmd-9-3199-2016._ | [Link](https://github.com/ccr/ccr) | Charlie Zender  (University of California, Irvine) |
| Granular BitRound (GBR)	| 32023	| **The GBG quantization algorithm is a significant improvement to the BitGroom filter** The GBG quantization algorithm is a significant improvement the BitGroom filter documented in: _Zender, C. S. (2016), Bit Grooming: Statistically accurate precision-preserving quantization with compression, evaluated in the netCDF Operators (NCO, v4.4.8+), Geosci. Model Dev., 9, 3199-3211, doi:10.5194/gmd-9-3199-2016._ | [Link](https://github.com/ccr/ccr) | Charlie Zender  (University of California, Irvine) |
| SZ3	| 32024	| **A modular error-bounded lossy compression framework for scientific datasets** SZ3 is a modular error-bounded lossy compression framework for scientific datasets, which allows users to customize their own compression pipeline to adapt to diverse datasets and user-requirements. Compared with SZ2 (filter id: 32017), SZ3 has integrated a more effective prediction such that its compression qualities/ratios are much higher than that of SZ2 in most of cases. | [Source](https://github.com/szcompressor/SZ3), [License](https://github.com/szcompressor/SZ/blob/master/copyright-and-BSD-license.txt) | [Sheng Di](mailto:sdi1@anl.gov), [Franck Cappello](mailto:cappello@mcs.anl.gov) |
| Delta-Rice | 32025 | **Lossless compression algorithm optimized for digitized analog signals based on delta encoding and rice coding** | [Link](https://gitlab.com/dgma224/deltarice) | [David Mathews](mailto:david.mathews.1994@gmail.com) |
| BLOSC | 32026	| **The recent new-generation version of the Blosc compression library** Blosc is a high performance compressor optimized for binary data (i.e. floating point numbers, integers and booleans). It has been designed to transmit data to the processor cache faster than the traditional, non-compressed, direct memory fetch approach via a memcpy() OS call. Blosc main goal is not just to reduce the size of large datasets on-disk or in-memory, but also to accelerate memory-bound computations. C-Blosc2 is the new major version of C-Blosc, and tries hard to be backward compatible with both the C-Blosc1 API and its in-memory format. | [Blosc project](https://www.blosc.org), [C-Blosc2 docs](https://www.blosc.org/c-blosc2/c-blosc2.html), [License](https://github.com/Blosc/c-blosc2/blob/main/LICENSE.txt) | [Francesc Alted (BDFL for the Blosc project)](mailto:faltet@gmail.com) |
| FLAC | 32027 | **FLAC audio compression filter in HDF5** FLAC is an audio compression filter in HDF5. (Our ultimate goal is to use it via `h5py` in the [`hdf5plugin` library](https://github.com/silx-kit/hdf5plugin)). | [Source](https://github.com/xiph/flac), [License](https://github.com/xiph/flac/blob/master/CONTRIBUTING.md) | [Laurie Stephey](mailto:lastephey@lbl.gov) |
